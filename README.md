

# ğŸ“Š Credit Card Fraud Detection â€“ Machine Learning Project

## ğŸ“Œ Project Overview

This project focuses on detecting fraudulent credit card transactions using **machine learning classification techniques**.
The objective is to accurately classify transactions as **fraudulent (1)** or **non-fraudulent (0)** despite severe **class imbalance**.

Multiple models were implemented, evaluated, and compared, followed by **model deployment** using a saved pipeline for real-time predictions.

---

## ğŸ“‚ Dataset Description

* **Dataset Name:** Credit Card Transactions Dataset
* **Source:** Public Kaggle Dataset
* **File:** `creditcard.csv`
* **Total Records:** ~284,807 transactions
* **Target Variable:**

  * `Class = 0` â†’ Non-Fraud
  * `Class = 1` â†’ Fraud
* **Features:**

  * `V1` to `V28`: PCA-transformed features
  * `Amount`: Transaction amount
  * `Time`: Time elapsed between transactions
  * `Class`: Target variable

âš ï¸ The dataset is **highly imbalanced**, with fraud cases accounting for less than **0.2%** of all transactions.

---

## ğŸ”§ Data Preprocessing

The following preprocessing steps were performed:

### 1ï¸âƒ£ Missing Values

* Checked for null values using `.isnull().sum()`
* Missing values were handled using **forward fill (ffill)** where applicable

### 2ï¸âƒ£ Duplicate Removal

* Duplicate records were identified and removed to maintain data integrity

### 3ï¸âƒ£ Feature Scaling

* **StandardScaler** was applied to numerical features to normalize data
* Scaling is critical for distance-based models like **KNN** and gradient-based models like **Logistic Regression**

### 4ï¸âƒ£ Class Imbalance Handling

* **Undersampling technique** was used:

  * Randomly sampled legitimate transactions to match fraud transaction count
* This ensured balanced training data for model learning

---

## ğŸ§  Feature Engineering

Feature engineering focused on improving predictive performance:

* **Transaction Amount Analysis**

  * Statistical comparison between fraud and non-fraud amounts
* **Correlation Analysis**

  * Heatmaps used to identify features most correlated with fraud (`V10`, `V12`, `V14`, `Amount`)
* **Feature Selection**

  * Reduced feature set used for deployment:

    * `Amount`, `V10`, `V12`, `V14`

These engineered features help capture **usage patterns and abnormal behavior** similar to churn prediction metrics.

---

## ğŸ¤– Model Selection & Methodology

Multiple classification models were trained and evaluated:

### ğŸ”¹ Logistic Regression

* Simple and interpretable baseline model
* Works well with scaled numerical features
* Used for final deployment due to stability and performance

### ğŸ”¹ Decision Tree Classifier

* Captures non-linear relationships
* Visualized using `plot_tree()`
* Risk of overfitting on imbalanced data

### ğŸ”¹ K-Nearest Neighbors (KNN)

* Distance-based classifier
* Optimal `k` chosen using **error-rate vs k** plot
* Computationally expensive on large datasets

### ğŸ”¹ Model Comparison

Models were compared using standardized metrics to identify the best performer.

---

## ğŸ“ Evaluation Metrics

The following evaluation metrics were used:

* **Accuracy** â€“ Overall correctness of the model
* **Precision** â€“ How many predicted frauds were actually fraud
* **Recall (Sensitivity)** â€“ Ability to detect actual fraud cases
* **F1-Score** â€“ Balance between Precision and Recall
* **Confusion Matrix** â€“ Detailed error analysis

ğŸ“Œ **F1-Score and Recall** were prioritized due to class imbalance.

---

## ğŸ“ˆ Model Performance Comparison

A comparison table was generated:

| Model               | Accuracy | Precision | Recall | F1-Score |
| ------------------- | -------- | --------- | ------ | -------- |
| Logistic Regression | âœ”        | âœ”         | âœ”      | âœ”        |
| KNN                 | âœ”        | âœ”         | âœ”      | âœ”        |
| Decision Tree       | âœ”        | âœ”         | âœ”      | âœ”        |

The **Logistic Regression model** showed the best trade-off between interpretability, recall, and F1-score.

---

## ğŸš€ Model Deployment

A production-ready pipeline was created using:

* **StandardScaler**
* **Logistic Regression**
* **Scikit-learn Pipeline**

### ğŸ”¹ Model Saving

```bash
fraud_detection_model.pkl
scaler.pkl
```

Saved using **joblib** for reuse.

---

## ğŸ§ª Real-Time Prediction

* Interactive prediction interface built using **ipywidgets**
* User inputs:

  * `Amount`
  * `V10`
  * `V12`
  * `V14`
* Model outputs:

  * `0` â†’ Not Fraud
  * `1` â†’ Fraud

---

## ğŸ›  Technologies Used

* Python
* Pandas, NumPy
* Matplotlib, Seaborn
* Scikit-learn
* Joblib
* ipywidgets

---

## ğŸ“Œ Conclusion

This project demonstrates a complete **end-to-end machine learning workflow**:

* Data preprocessing
* Feature engineering
* Model training & evaluation
* Handling class imbalance
* Model comparison
* Deployment & real-time inference

It mirrors **churn prediction methodologies**, focusing on behavior analysis, imbalance handling, and recall-driven evaluation.

---

